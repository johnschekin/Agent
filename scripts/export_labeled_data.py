#!/usr/bin/env python3
"""Export labeled evidence rows to JSONL/Parquet with lineage metadata.

Usage:
    python3 scripts/export_labeled_data.py \
      --inputs workspaces/indebtedness/evidence \
      --output-prefix workspaces/indebtedness/results/labeled_data
"""

from __future__ import annotations

import argparse
import json
import sys
from datetime import UTC, datetime
from pathlib import Path
from typing import Any
from uuid import uuid4

try:
    import orjson

    def dump_json(obj: Any) -> None:
        sys.stdout.buffer.write(orjson.dumps(obj, option=orjson.OPT_INDENT_2))
        sys.stdout.buffer.write(b"\n")

    def load_json(path: Path) -> Any:
        return orjson.loads(path.read_bytes())

    def dump_json_bytes(obj: Any) -> bytes:
        return orjson.dumps(obj, default=str)
except ImportError:

    def dump_json(obj: Any) -> None:
        json.dump(obj, sys.stdout, indent=2, default=str)
        print()

    def load_json(path: Path) -> Any:
        return json.loads(path.read_text())

    def dump_json_bytes(obj: Any) -> bytes:
        return json.dumps(obj, default=str).encode("utf-8")


def log(msg: str) -> None:
    print(msg, file=sys.stderr)


def _load_rows_from_file(path: Path) -> list[dict[str, Any]]:
    suffix = path.suffix.lower()
    rows: list[dict[str, Any]] = []
    if suffix == ".jsonl":
        for line in path.read_text().splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                payload = json.loads(line)
            except json.JSONDecodeError:
                continue
            if isinstance(payload, dict):
                rows.append(payload)
        return rows

    payload = load_json(path)
    if isinstance(payload, list):
        return [row for row in payload if isinstance(row, dict)]
    if isinstance(payload, dict):
        for key in ("rows", "matches", "hits", "results", "evidence"):
            val = payload.get(key)
            if isinstance(val, list):
                return [row for row in val if isinstance(row, dict)]
    return []


def _collect_input_files(inputs: list[str]) -> list[Path]:
    files: list[Path] = []
    for raw in inputs:
        path = Path(raw)
        if path.is_file():
            files.append(path)
            continue
        if path.is_dir():
            files.extend(sorted(path.glob("*.jsonl")))
            files.extend(sorted(path.glob("*.json")))
    # Deduplicate while preserving order.
    deduped: list[Path] = []
    seen: set[Path] = set()
    for fp in files:
        resolved = fp.resolve()
        if resolved in seen:
            continue
        seen.add(resolved)
        deduped.append(fp)
    return deduped


def _default_manifest_path_for_db(db_path: Path) -> Path:
    p = db_path
    if p.suffix.lower() == ".duckdb":
        return p.with_suffix(".run_manifest.json")
    return p.parent / "run_manifest.json"


def _to_text(value: Any) -> str:
    if value is None:
        return ""
    return str(value)


def _to_number(value: Any) -> Any:
    if value is None:
        return None
    try:
        if isinstance(value, bool):
            return int(value)
        if isinstance(value, int | float):
            return value
        txt = str(value).strip()
        if txt == "":
            return None
        if "." in txt:
            return float(txt)
        return int(txt)
    except Exception:
        return value


def _jsonable(value: Any) -> Any:
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if isinstance(value, list):
        return [_jsonable(v) for v in value]
    if isinstance(value, dict):
        return {str(k): _jsonable(v) for k, v in value.items()}
    return str(value)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Export labeled evidence rows to JSONL/Parquet with lineage fields."
    )
    parser.add_argument(
        "--inputs",
        nargs="+",
        required=True,
        help="Input evidence files or directories (JSONL/JSON).",
    )
    parser.add_argument(
        "--output-prefix",
        required=True,
        help="Output file prefix (without extension).",
    )
    parser.add_argument(
        "--format",
        choices=("jsonl", "parquet", "both"),
        default="both",
        help="Export format (default: both).",
    )
    parser.add_argument(
        "--source-db",
        default=None,
        help="Optional source corpus DB path for lineage metadata.",
    )
    parser.add_argument(
        "--run-id",
        default=None,
        help="Optional export run identifier (auto-generated by default).",
    )
    parser.add_argument(
        "--strategy-version",
        type=int,
        default=None,
        help="Optional override for strategy_version lineage field.",
    )
    parser.add_argument(
        "--ontology-node-id",
        default=None,
        help="Optional override for ontology_node_id lineage field.",
    )
    parser.add_argument(
        "--include-not-found",
        action="store_true",
        help="Include NOT_FOUND records (default: HIT records only).",
    )
    parser.add_argument(
        "--dedupe",
        action="store_true",
        help="Deduplicate rows by (ontology_node_id, doc_id, section_number, clause_path, char_start, char_end).",
    )
    args = parser.parse_args()

    input_files = _collect_input_files(args.inputs)
    if not input_files:
        log("Error: no evidence files found from --inputs")
        sys.exit(1)

    source_db = Path(args.source_db).resolve() if args.source_db else None
    source_manifest_path = (
        _default_manifest_path_for_db(source_db) if source_db is not None else None
    )
    source_manifest_exists = bool(source_manifest_path and source_manifest_path.exists())

    export_run_id = args.run_id or (
        f"export_labeled_{datetime.now(UTC).strftime('%Y%m%dT%H%M%SZ')}_{uuid4().hex[:8]}"
    )
    exported_at = datetime.now(UTC).isoformat()

    rows: list[dict[str, Any]] = []
    file_row_counts: dict[str, int] = {}
    for fp in input_files:
        loaded = _load_rows_from_file(fp)
        file_row_counts[str(fp)] = len(loaded)
        for row in loaded:
            if not isinstance(row, dict):
                continue
            record_type = str(row.get("record_type", "HIT")).upper().strip()
            if record_type == "NOT_FOUND" and not args.include_not_found:
                continue
            enriched = dict(row)
            if args.strategy_version is not None:
                enriched["strategy_version"] = args.strategy_version
            if args.ontology_node_id:
                enriched["ontology_node_id"] = args.ontology_node_id
                enriched["concept_id"] = args.ontology_node_id
            enriched["record_type"] = record_type
            enriched["lineage"] = {
                "export_run_id": export_run_id,
                "exported_at": exported_at,
                "source_evidence_file": str(fp),
                "source_db": str(source_db) if source_db else "",
                "source_db_manifest": str(source_manifest_path) if source_manifest_path else "",
                "source_db_manifest_exists": source_manifest_exists,
                "source_run_id": _to_text(enriched.get("run_id", "")),
                "source_strategy_version": _to_number(enriched.get("strategy_version")),
            }
            rows.append(enriched)

    deduped_count = 0
    if args.dedupe:
        key_fields = (
            "ontology_node_id",
            "doc_id",
            "section_number",
            "clause_path",
            "char_start",
            "char_end",
            "record_type",
        )
        unique_rows: list[dict[str, Any]] = []
        seen: set[tuple[Any, ...]] = set()
        for row in rows:
            key = tuple(row.get(k) for k in key_fields)
            if key in seen:
                deduped_count += 1
                continue
            seen.add(key)
            unique_rows.append(row)
        rows = unique_rows

    out_prefix = Path(args.output_prefix)
    out_prefix.parent.mkdir(parents=True, exist_ok=True)

    jsonl_path: Path | None = None
    parquet_path: Path | None = None
    parquet_error = ""

    if args.format in {"jsonl", "both"}:
        jsonl_path = out_prefix.with_suffix(".jsonl")
        with jsonl_path.open("wb") as f:
            for row in rows:
                f.write(dump_json_bytes(_jsonable(row)))
                f.write(b"\n")

    if args.format in {"parquet", "both"}:
        parquet_path = out_prefix.with_suffix(".parquet")
        try:
            import pyarrow as pa  # type: ignore[import-not-found]
            import pyarrow.parquet as pq  # type: ignore[import-not-found]
        except Exception as exc:  # pragma: no cover - environment-dependent
            parquet_error = str(exc)
        else:
            columns: dict[str, list[Any]] = {}
            for row in rows:
                for key in row.keys():
                    columns.setdefault(str(key), [])
            for key in columns.keys():
                for row in rows:
                    value = row.get(key)
                    if isinstance(value, (dict, list)):
                        columns[key].append(json.dumps(_jsonable(value), sort_keys=True))
                    else:
                        columns[key].append(_jsonable(value))
            table = pa.table(columns)
            pq.write_table(table, parquet_path)

    output = {
        "schema_version": "labeled_export_v1",
        "status": "ok" if not parquet_error else "partial",
        "export_run_id": export_run_id,
        "exported_at": exported_at,
        "inputs": [str(fp) for fp in input_files],
        "input_file_counts": file_row_counts,
        "rows_exported": len(rows),
        "deduped_rows": deduped_count,
        "include_not_found": bool(args.include_not_found),
        "source_db": str(source_db) if source_db else "",
        "source_db_manifest": str(source_manifest_path) if source_manifest_path else "",
        "source_db_manifest_exists": source_manifest_exists,
        "jsonl_path": str(jsonl_path) if jsonl_path else "",
        "parquet_path": str(parquet_path) if parquet_path and not parquet_error else "",
    }
    if parquet_error:
        output["parquet_error"] = parquet_error
    dump_json(output)


if __name__ == "__main__":
    main()
